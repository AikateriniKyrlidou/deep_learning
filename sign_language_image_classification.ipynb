{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Sign language is a visual mean of communication that uses gestures, body language and different facial expressions in order to allow those who use it to communicate with one another. It is mainly used by deaf or hard-of-hearing people all over the world. Just like any other spoken language there are many distinct sing languages. This particular dataset is about the American Sign Language (ASL). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The [dataset](https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset) is an image dataset (the images are in nympy array format). It consists of many different images showing letters and numbers of the ASL, provided by 173 individuals. The images belong to a total of 27 classes as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HTML for the lists\n",
    "html_content = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <div>\n",
    "    <ul>\n",
    "      <li> '0'</li>\n",
    "      <li> '1'</li>\n",
    "      <li> '2'</li>\n",
    "      <li> '3'</li>\n",
    "      <li> '4'</li>\n",
    "      <li> '5'</li>\n",
    "      <li> '6'</li>\n",
    "      <li> '7'</li>\n",
    "      <li> '8'</li>\n",
    "      <li> '9'</li>\n",
    "      <li> 'NULL'</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div>\n",
    "    <ul>\n",
    "      <li>'a'</li>\n",
    "      <li>'b'</li>\n",
    "      <li>'bye'</li>\n",
    "      <li>'c'</li>\n",
    "      <li>'d'</li>\n",
    "      <li>'e'</li>\n",
    "      <li>'good'</li>\n",
    "      <li>'good morning'</li>\n",
    "      <li>'hello'</li>\n",
    "      <li>'little bit'</li>\n",
    "      <li>'no'</li>\n",
    "      <li>'pardon'</li>\n",
    "      <li>'please'</li>\n",
    "      <li>'project'</li>\n",
    "      <li>'whats up'</li>\n",
    "      <li>'yes'</li>\n",
    "\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML content\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous work done on the dataset\n",
    "There has been significant work done on this specific problem, using different approaches such as [Two-streamed mixed CNN](https://www.mdpi.com/1424-8220/22/16/5959) which reached 97.57% accuracy.\n",
    "\n",
    "Another study using [Deep Learning Technology](https://www.mdpi.com/1424-8220/23/18/7970) managed to provide great insight and results reaching 94-99% accuracies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The first thing we must do is import the necessary libraries. These libraries are needed to modify, visualize and process our data and results in order to understand both the problem we are trying to solve and its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #for numerical computations and array manipulations.\n",
    "import pandas as pd #data manipulation and analysis\n",
    "import os    #used for file and directory operations\n",
    "import random\n",
    "import keras  #used to build and train deep learning models.\n",
    "import sklearn\n",
    "import seaborn as sns #data visualization used for making statistical graphics.\n",
    "import cv2  #used for image processing\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt #for graphs and images\n",
    "from PIL import Image #for opening, manipulating, and saving images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder  #converting categorical labels into a numerical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, jaccard_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import convert_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images\n",
    "\n",
    "The images of our dataset are stored in numpy files which are widely used for storing arrays, therefore we must upload the two files, namely \"X.npy\" and \"Y.npy\". The first file contains all of the images/features while the second one consists of the different labels that describe what each gesture portrayed symbolizes. \n",
    "\n",
    "It is important to note that the specific format of the dataset is very convenient since it is compatible with a framework like Keras which we will use, without needing any further alterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the directory where the dataset is\n",
    "input_dir = os.path.join(\"27_class_sign_language_dataset\")\n",
    "#the full paths to each one of the files containing the images and the labels\n",
    "x_filename = os.path.join(input_dir, \"X.npy\")\n",
    "y_filename = os.path.join(input_dir, \"Y.npy\")\n",
    "\n",
    "#loading the data from the .npy files\n",
    "x = np.load(x_filename)\n",
    "y = np.load(y_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show some samples in order to see if the images were properly loaded as well as their resolution, labels etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a figure with 5 subplots while also setting the sizes\n",
    "fig, axes = plt.subplots(1, 5, figsize=(13, 4))\n",
    "\n",
    "#ensuring that the arrays containing the images and labels have the same size\n",
    "assert x.shape[0] == y.shape[0], \"The image and label arrays are not of the same size\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randrange(1, 22000)\n",
    "    #display the image with the index r \n",
    "    axes[i].imshow(x[r])\n",
    "    #set the title of the subplot to be the label of index r\n",
    "    axes[i].set_title(y[r])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "#making sure each subplot fits within the figure area\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing\n",
    "\n",
    "The resolution of our images seems to be 128x128, converting it to 64x64 while maintaining the RGB feature, seems to produce adequate results while also keeping the computational demands relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the new dimensions for resizing\n",
    "new_height = 64\n",
    "new_width = 64\n",
    "\n",
    "#initialize an array to store resized images\n",
    "X = np.empty((x.shape[0], new_height, new_width, x.shape[3]), dtype=x.dtype)\n",
    "\n",
    "#resize each image using OpenCV\n",
    "for i, image in enumerate(x):\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))  \n",
    "    X[i] = resized_image  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will show some of the resized images to ensure that the process was completed succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a figure with 5 subplots while also setting the sizes\n",
    "fig, axes = plt.subplots(1, 5, figsize=(13, 4))\n",
    "\n",
    "#ensuring that the arrays containing the images and labels have the same size\n",
    "assert x.shape[0] == y.shape[0], \"The image and label arrays are not of the same size\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randrange(1, 22000)\n",
    "    #display the image with the index r \n",
    "    axes[i].imshow(X[r])\n",
    "    #set the title of the subplot to be the label of index r\n",
    "    axes[i].set_title(y[r])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "#making sure each subplot fits within the figure area.\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "The total amount of images that have been loaded from the previous steps must be split into train and test sets. The first will be used to fit the model and the model and the second to evaluate the results of the fitting. We chose 80-20% train-test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"x_train\" now contains the images that will be used for training, \"y_train\" contains their corresponding labels. The images and labels that will be used for training are contained in \"x_test\" and \"y_test\" accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] == y_train.shape[0], \"Number of images and labels must match\"\n",
    "\n",
    "print(\"Image shape:\", x_train.shape[1:]) \n",
    "print(\"Train set size:\", x_train.shape[0])\n",
    "print(\"Test set size:\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are 18240 images in the train set and 4561 images in the test set. Also the shape of the RGB images is now (64, 64, 3) showing that the size was indeed reduced to half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation \n",
    "\n",
    "We will perform data augmentation using [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). More specifically we will rotate, shift, shear, zoom into and flip our images. This provides us with many altered versions of each of the original images. Enriching the training set ensures better training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an ImageDataGenerator instance with the desired augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    #randomly rotate in the range (0 to 180)  \n",
    "    rotation_range=20,       \n",
    "    #randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1, \n",
    "    #randomly shift images vertically (fraction of total height)  \n",
    "    height_shift_range=0.1,  \n",
    "    #shear intensity (counter-clockwise direction in degrees)\n",
    "    shear_range=0.2,  \n",
    "    #randomly zoom into images       \n",
    "    zoom_range=0.2,      \n",
    "    #randomly flip images horizontally    \n",
    "    horizontal_flip=True,\n",
    "    #randomly flip images vertically\n",
    "    vertical_flip=True      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply the transformations that we defined above to our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)\n",
    "\n",
    "#configure batch size and generate augmented data\n",
    "batch_size = 32\n",
    "augmented_data_generator = datagen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show some examples of the augmented images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of iterating through augmented batches and displaying images\n",
    "for i in range(1):  #loop over one batch\n",
    "    batch_images, batch_labels = augmented_data_generator[i] \n",
    "    \n",
    "    #plotting the first 9 images in the batch\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for j in range(9):\n",
    "        plt.subplot(3, 3, j + 1)\n",
    "        plt.imshow(batch_images[j])  \n",
    "        plt.title(f\"Label: {batch_labels[j]}\")  \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalizing our data helps improve the stability and performance of the model by eliminating absolute values and inherent biases.\n",
    "\n",
    "We will print the value interval of our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Value interval of the train set :[{x_train.min()},{x_train.max()}]\")\n",
    "print(f\"Value interval of the test set :[{x_test.min()},{x_test.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our images are already in the (0,1) range so there is no need for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "CNNs are mostly used for image data, since convolutional operations detect the patterns that exist in the input images and such patterns can be recognized regardless of where they are located in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "In the given problem the gestures are positioned in the center of the image and have characteristics that the model can learn to recognize. The pooling layers are used to reduce the spatial dimensions and force the model to retain the most important features.\n",
    "\n",
    "The dropout layers are used to prevent overfitting. The flatten layer transforms the data into a vector that will then be used as input to the dense layer in order to produce the final probability for each class that each image can belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.Sequential([keras.layers.Conv2D(filters=64, \n",
    "                                                        kernel_size=(5,5), \n",
    "                                                        padding=\"same\", \n",
    "                                                        activation=\"relu\", \n",
    "                                                        input_shape=(64,64,3)),   #this is the size of my images\n",
    "                                     keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "                                     keras.layers.Conv2D(filters=128,\n",
    "                                                        kernel_size=(5,5),\n",
    "                                                        padding=\"same\",\n",
    "                                                        activation=\"relu\"),\n",
    "                                     keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "                                     keras.layers.Conv2D(filters=256,\n",
    "                                                        kernel_size=(5,5),\n",
    "                                                        padding=\"same\",\n",
    "                                                        activation=\"relu\"),    \n",
    "                                     keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "                                     keras.layers.Dropout(rate=0.25),\n",
    "                                     keras.layers.Flatten(), \n",
    "                                     keras.layers.Dropout(rate=0.5),\n",
    "                                     keras.layers.Dense(units=256, \n",
    "                                                        activation=\"relu\"),\n",
    "                                     #using softmax to get the inputs as a probability distribution \n",
    "                                     keras.layers.Dense(units=27,   #this is the number of classes\n",
    "                                                        activation=\"softmax\")\n",
    "                                    ]\n",
    "                                   ) \n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a summary of our model and the total number of parameters that it uses.\n",
    "\n",
    "We compile our model using the \"adam\" optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(\n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=[\"accuracy\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "First we will convert our labels to categorical in order to train and evalute our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(y_train)\n",
    "test_labels_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "#print the number of unique classes\n",
    "num_classes = len(np.unique(train_labels_encoded))\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "#convert numeric labels to categorical (one-hot encoded) values\n",
    "train_labels_categorical = keras.utils.to_categorical(train_labels_encoded, num_classes=num_classes)\n",
    "test_labels_categorical = keras.utils.to_categorical(test_labels_encoded, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train our model using the train set and splitting a validation set of 20% from it in order to continuously evaluate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process using fit()\n",
    "history = cnn_model.fit(x_train, \n",
    "                       train_labels_categorical,   \n",
    "                       epochs=30,\n",
    "                       batch_size=128,\n",
    "                       validation_split=0.2, \n",
    "                       verbose = 0\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The following plots show the 'Cross-Entropy' loss and the 'Accuracy' that are used to assess the model through the different epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n",
    "\n",
    "axis[0].plot(history.epoch, history.history['loss']) \n",
    "axis[0].plot(history.epoch, history.history['val_loss']) \n",
    "axis[0].set_xlabel(\"Epochs\") \n",
    "axis[0].set_ylabel(\"Value\") \n",
    "axis[0].legend([\"CE\", \"Val_CE\"]) \n",
    "axis[0].set_title(\"Training Process - CE\") \n",
    "\n",
    "axis[1].plot(history.epoch, history.history['accuracy'])\n",
    "axis[1].plot(history.epoch, history.history['val_accuracy'])\n",
    "axis[1].set_xlabel(\"Epochs\") \n",
    "axis[1].set_ylabel(\"Value\") \n",
    "axis[1].legend([\"Accuracy\", \"Val_Accuracy\"])\n",
    "axis[1].set_title(\"Training Process - Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our model using several metrics, namely 'accuracy', 'precision', 'recall' and 'f1-score'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model makes a prediction with probabilities\n",
    "predictions_nn_probs = cnn_model.predict(x_test, verbose = 0)\n",
    "\n",
    "#each prediction is the maximum of the probabilities\n",
    "predictions_nn = np.argmax(predictions_nn_probs, axis=1)\n",
    "\n",
    "test_accuracy_nn = sklearn.metrics.accuracy_score(test_labels_encoded, predictions_nn)\n",
    "\n",
    "print(\"The Accuracy of the Neural Network on the Test Data is\", test_accuracy_nn)\n",
    "\n",
    "print(classification_report(test_labels_encoded, predictions_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the classification report we can see the metrics for each class of the dataset.\n",
    "\n",
    "We will also produce a confusion matrix in order to gain a better understanding of the results of the classification process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model.predict(x_test, verbose = 0)\n",
    "y_pred_labels = np.argmax(y_pred, axis=-1)  \n",
    "\n",
    "class_names = np.unique(y)\n",
    "\n",
    "#create the confusion matrix\n",
    "cm = confusion_matrix(test_labels_encoded, y_pred_labels)\n",
    "\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#create a figure and axes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes()\n",
    "\n",
    "#define my own color palette\n",
    "palette = sns.color_palette('ch:s=-.2,r=.6')\n",
    "\n",
    "#plot the confusion matrix\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".1f\", cmap=sns.cubehelix_palette(as_cmap=True), xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "\n",
    "#set labels, title, and ticks\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(class_names)\n",
    "ax.yaxis.set_ticklabels(class_names)\n",
    "\n",
    "#rotate x-axis tick labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "#display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes evident from the accuracy as well as from the confusion matrix that for almost all of the classes, the labels that were correctly predicted are close to 96-97%. The model appears to make certain false predictions when trying to classify images of gestures that look a lot like one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model with randomly selected images from the test set\n",
    "num_images_to_display = 10  # Number of images to display\n",
    "#ensure different images with each run\n",
    "indices_to_test = np.random.choice(len(x_test), size=num_images_to_display, replace=False)\n",
    "\n",
    "#set up the figure for plotting images in a grid\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "#iterate through the randomly selected indices of the test set\n",
    "for i, idx in enumerate(indices_to_test):\n",
    "    # Get the image and true label from the test set\n",
    "    img = x_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    #predict the label for the image using the trained CNN model\n",
    "    prediction_probs = cnn_model.predict(np.expand_dims(img, axis=0))\n",
    "    predicted_label = np.argmax(prediction_probs)\n",
    "\n",
    "    #plot each image in a subplot\n",
    "    plt.subplot(2, 5, i + 1)  #2 rows, 5 columns for the grid of images\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True: {true_label}, Predicted: {true_label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "#adjust layout and display\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The classification of sign language images is a task that seems to be achievable by relatively simple CNNs. Models that perform this type of classification can be incorporated in applications that assist people who use sign language in their day-to-day interactions, making communication faster and easier for all involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
